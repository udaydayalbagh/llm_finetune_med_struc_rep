import logging
from src.models.model import generate_output
from src.models.reward import structured_report_reward
import yaml

logger = logging.getLogger(__name__)

def evaluate_output(output_text: str) -> dict:
    """
    Evaluate a generated output for compliance with the expected Neo4j format.

    Parameters:
        output_text (str): The text generated by the model.

    Returns:
        dict: Evaluation metrics including a reward score and validity flag.
    """
    # TODO: Add evaluation metrics
    try:
        reward = structured_report_reward(output_text)
        # Example threshold; adjust as needed based on your domain requirements.
        is_valid = reward >= 2.5
        metrics = {
            "reward": reward,
            "is_valid": is_valid
        }
        logger.debug(f"Evaluation metrics: {metrics}")
        return metrics
    except Exception as e:
        logger.exception(f"Error during output evaluation: {e}")
        return {"reward": 0.0, "is_valid": False}


def evaluate_model(model_tuple, data: list, max_length: int = 256) -> list:
    """
    Evaluate the model on a dataset of medical reports.
    
    Parameters:
        model_tuple (tuple): A tuple (model, tokenizer) for generating outputs.
        data (list): A list of report dictionaries containing a 'text' field.
        max_length (int): Maximum length for generated outputs.
    
    Returns:
        list: A list of evaluation results, each including the input, output, and metrics.
    """
    evaluations = []
    for report in data:
        input_text = report.get("text", "")
        if not input_text:
            logger.warning("Skipping report with empty text during evaluation.")
            continue
        try:
            with open("config/config.yaml", "r") as f:
                config = yaml.safe_load(f)
            system_prompt = config["system_prompt"]
            text = system_prompt + "\n\n" + input_text
            
            output_text = generate_output(model_tuple, text, max_length)
            metrics = evaluate_output(output_text)
            evaluations.append({
                "input": input_text,
                "output": output_text,
                "metrics": metrics
            })
        except Exception as e:
            logger.exception(f"Error evaluating report: {e}")
            evaluations.append({
                "input": input_text,
                "output": None,
                "metrics": {"reward": 0.0, "is_valid": False}
            })
    return evaluations
